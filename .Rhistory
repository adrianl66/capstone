library(tm)
library(RWeka)
library(tau)
library(wordcloud)
library(slam)
library(stringi)
library(R.utils)
library(dplyr)
library(ggplot2)
library(knitr)
install.packages("tm")
install.packages("RWeka")
install.packages("tau")
install.packages("wordcloud")
install.packages("slam")
install.packages("stringi")
install.packages("R.utils")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("knitr")
library(tm)
library(RWeka)
library(tau)
library(wordcloud)
library(slam)
library(stringi)
library(R.utils)
library(dplyr)
library(ggplot2)
library(knitr)
library(tm)
library(RWeka)
library(tau)
library(wordcloud)
library(slam)
library(stringi)
library(R.utils)
library(dplyr)
library(ggplot2)
library(knitr)
library(tm)
library(RWeka)
library(tau)
library(wordcloud)
library(slam)
library(stringi)
library(R.utils)
library(dplyr)
library(ggplot2)
library(knitr)
install.packages("RWeka")
library(RWeka)
library(tm)
library(RWeka)
library(tau)
library(wordcloud)
library(SnowballC)
library(slam)
library(stringi)
library(R.utils)
library(dplyr)
library(ggplot2)
library(knitr)
setwd("C:/Users/adrian/Desktop/Git Repository/COursera/capstone")
blogs <- readLines("./download/en_US.blogs.txt",encoding='UTF-8',skipNul=TRUE)
#iconv(blogs, "UTF-8", "ascii", sub = " ")
news <- readLines("./download/en_US.news.txt",encoding='UTF-8',skipNul=TRUE)
#iconv(news, "UTF-8", "ascii", sub = " ")
twitter <- readLines("./download/en_US.twitter.txt",encoding='UTF-8',skipNul=TRUE)
#iconv(twitter, "UTF-8", "ascii", sub = " ")
set.seed(80) #enable reproducibility
Tblogs <- blogs[sample(1:length(blogs),0.1*length(blogs))]
Tnews <- news[sample(1:length(news),0.1*length(news))]
#Ttwitter <- twitter[sample(1:length(twitter),0.05*length(twitter))]
writeLines(Testfile,"./data/TestFile.txt")
Cleanfile <- Corpus(DirSource("./data"))
Testfile <- c(Tblogs,Tnews)
writeLines(Testfile,"./data/TestFile.txt")
Cleanfile <- Corpus(DirSource("./data"))
Cleanfile <- Corpus(VectorSource(Cleanfile))
Cleanfile <- tm_map(Cleanfile,removePunctuation)
Cleanfile <- tm_map(Cleanfile,removeNumbers)
Cleanfile <- tm_map(Cleanfile,stripWhitespace)
Cleanfile <- tm_map(Cleanfile,content_transformer(tolower))
Cleanfile <- tm_map(Cleanfile,stemDocument)
Cleanfile <- tm_map(Cleanfile,removeWords, stopwords("english"))
#Read in list of profanity words that we want to remove
profanity <- readLines("./download/profanity.txt",encoding='UTF-8',skipNul=TRUE)
Cleanfile <- tm_map(Cleanfile,removeWords, profanity)
#Cleanfile <- removeSparseTerms(Cleanfile , 0.9995)
Cleanfile <- tm_map(Cleanfile,PlainTextDocument)
save(Cleanfile,file="./corpus/Corpus.RData")
head(Cleanfile[[1]]$content, n = 10)
tdm <- TermDocumentMatrix(Cleanfile)
tdm
summary(Cleanfile)
library(quanteda)
if (sum(ls() == "qcorpus") < 1) {
qcorpus <- corpus(Cleanfile)
}
summary(qcorpus, 10)
qcorpus <- corpus(Cleanfile)
ng1 <- tokenize(Cleanfile, removePunct = TRUE, ngrams = 1)
Unigram <- function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
UniDoc <- DocumentTermMatrix(Cleanfile,control=list(tokenize = Unigram))
Bigram <- function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
BiDoc <- DocumentTermMatrix(Cleanfile,control=list(tokenize = Bigram))
Trigram <- function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
TriDoc <- DocumentTermMatrix(Cleanfile,control=list(tokenize = Trigram))
library(slam)
freq <- rowapply_simple_triplet_matrix(UniDoc,sum)
freqbi <- rowapply_simple_triplet_matrix(BiDoc,sum)
freqtri <- rowapply_simple_triplet_matrix(TriDoc,sum)
head(freq)
freq
firstname <- sapply(strsplit(names(freqbi), ' '), function(a) a[1])
secname <- sapply(strsplit(names(freqbi), ' '), function(a) a[2])
firsttriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[1])
sectriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[2])
tritriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[3])
unigramDF <- data.frame(names(freq),freq,stringsAsFactors = F)
bigramDF <- data.frame(names(freqbi),freqbi,firstname,secname,stringsAsFactors = F)
trigramDF <- data.frame(names(freqtri),freqtri,paste(firsttriname,sectriname),tritriname,stringsAsFactors = F)
names(unigramDF) <- c('unigram','freq')
names(bigramDF) <- c('bigram','freq','unigram','name')
names(trigramDF) <- c('trigram','freq','bigram','name')
save(unigramDF,bigramDF,trigramDF,file = './data/ngram.RData')
head(bigramDF)
bigramDF
predict0 <-function(input,badwords,unigramDF, bigramDF, trigramDF, maxResults = 3) {
sw <- stopwords(kind = "en")
input <- removePunctuation(input)
input <- removeNumbers(input)
input <- rev(unlist(strsplit(input," ")))
input <- setdiff(input,sw)
input <- input[grepl('[[:alpha:]]',input)]
input <- paste(input[2],input[1],sep = ' ')
input <- tolower(input)
if(input == ''|input == "na na") return('Warning: Just input something')
seektri<-grepl(paste0("^",input,"$"),trigramDF$bigram)
subtri<-trigramDF[seektri,]
input2 <- unlist(strsplit(input," "))[2]
seekbi <- grepl(paste0("^",input2,"$"),bigramDF$unigram)
subbi <- bigramDF[seekbi,]
unigramDF$s <- unigramDF$freq/nrow(unigramDF)*0.16
useuni <- unigramDF[order(unigramDF$s,decreasing = T),]
useunia <- useuni[1:maxResults,]
if (sum(seektri) == 0) {
if(sum(seekbi)==0){
return(head(unigramDF[order(unigramDF$freq,decreasing = T),1],
maxResults))
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)
names <- c(subbi$name,useunia$unigram)
score <- c(subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- setdiff(final,badwords)
final <- final[grepl('[[:alpha:]]',final)]
return(final[1:maxResults])
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)
subtri$s <- subtri$freq/sum(subtri$freq)
names <- c(subtri$name,subbi$name,useunia$unigram)
score <- c(subtri$s,subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- final[1:maxResults]
final <- setdiff(final,badwords)
final <- final[grepl('[[:alpha:]]',final)]
return(final)
}
predict0("case of")
predict0("case of",badwords,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("case of","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0()
predict0 <-function(input,badwords,unigramDF, bigramDF, trigramDF, maxResults = 3) {
#sw <- stopwords(kind = "en")
input <- removePunctuation(input)
input <- removeNumbers(input)
input <- rev(unlist(strsplit(input," ")))
#input <- setdiff(input,sw)
input <- input[grepl('[[:alpha:]]',input)]
input <- paste(input[2],input[1],sep = ' ')
input <- tolower(input)
if(input == ''|input == "na na") return('Warning: Just input something')
seektri<-grepl(paste0("^",input,"$"),trigramDF$bigram)
subtri<-trigramDF[seektri,]
input2 <- unlist(strsplit(input," "))[2]
seekbi <- grepl(paste0("^",input2,"$"),bigramDF$unigram)
subbi <- bigramDF[seekbi,]
unigramDF$s <- unigramDF$freq/nrow(unigramDF)*0.16
useuni <- unigramDF[order(unigramDF$s,decreasing = T),]
useunia <- useuni[1:maxResults,]
if (sum(seektri) == 0) {
if(sum(seekbi)==0){
return(head(unigramDF[order(unigramDF$freq,decreasing = T),1],
maxResults))
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)
names <- c(subbi$name,useunia$unigram)
score <- c(subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- setdiff(final,badwords)
final <- final[grepl('[[:alpha:]]',final)]
return(final[1:maxResults])
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)
subtri$s <- subtri$freq/sum(subtri$freq)
names <- c(subtri$name,subbi$name,useunia$unigram)
score <- c(subtri$s,subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- final[1:maxResults]
final <- setdiff(final,badwords)
final <- final[grepl('[[:alpha:]]',final)]
return(final)
}
predict0("case of","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("the","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("car","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("run","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("girl","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("help me","fuck",unigramDF, bigramDF, trigramDF, maxResults = 3)
corpus < Cleanfile
corpus <- Cleanfile
unigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 3, max = 3))
dir()
getwd()
unigram <- TermDocumentMatrix(corpus, control=list(tokenize=unigram_token))
save(unigram,file="unigram.RData")
bigram <- TermDocumentMatrix(corpus, control=list(tokenize=bigram_token))
save(bigram,file="bigram.RData")
trigram <- TermDocumentMatrix(corpus, control=list(tokenize=trigram_token))
save(trigram,file="trigram.RData")
library(tm)
library(slam)
freq <- rowapply_simple_triplet_matrix(unigram,sum)
freqbi <- rowapply_simple_triplet_matrix(bigram,sum)
freqtri <- rowapply_simple_triplet_matrix(trigram,sum)
save(freq,file="freq.RData")
save(freqbi,file="freqbi.RData")
save(freqtri ,file="freqtri.RData")
#sort n-grams by frequency
freq_uni <- sort(freq, decreasing = T)
freq_bi <- sort(freqbi, decreasing = T)
freq_tri <- sort(freqtri, decreasing = T)
#to visualize the frequency
barplot(freq_uni[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
barplot(freq_uni[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
barplot(freq_bi[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
barplot(freq_tri[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
firstname <- sapply(strsplit(names(freqbi), ' '), function(a) a[1])
secname <- sapply(strsplit(names(freqbi), ' '), function(a) a[2])
firsttriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[1])
sectriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[2])
tritriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[3])
names(freqbi)[1] #view first entry in the bi-gram model
firstname[1]#view the split of the first entry in the bi-gram model into the first word
secname[1]#view the split of the first entry in the bi-gram model into the second  word
#save uni, bi and trigrams as dataframes
unigramDF <- data.frame(names(freq),freq,stringsAsFactors = F)
bigramDF <- data.frame(names(freqbi),freqbi,firstname,secname,stringsAsFactors = F)
trigramDF <- data.frame(names(freqtri),freqtri,paste(firsttriname,sectriname),tritriname,stringsAsFactors = F)
names(unigramDF) <- c('unigram','freq')
names(bigramDF) <- c('bigram','freq','unigram','name')
names(trigramDF) <- c('trigram','freq','bigram','name')
save(unigramDF,bigramDF,trigramDF,file = 'ngram.RData')
View(unigramDF)
View(bigramDF)
View(unigramDF)
predict0 <-function(input,profanity,unigramDF, bigramDF, trigramDF, maxResults = 3) {
input <- removePunctuation(input)#remove punctuation
input <- removeNumbers(input)#remove number
input <- tolower(input)#to lower case
#input <- stemDocument(input)#hmmm, get rid?
input <- stripWhitespace(input)
input <- input[grepl('[[:alpha:]]',input)]
if(input == ''|input == "na na") stop ('Warning: Just input something')#figure out a more graceful way of exiting
seektri<-grepl(paste0("^",input,"$"),trigramDF$bigram)#match input to bigram component of trigram
subtri<-trigramDF[seektri,]#retrieve matched trigram if present, retrieve header if not present
input2 <- unlist(strsplit(input," "))[2]# retrieve matched trigram if present,retrieve header if not present
seekbi <- grepl(paste0("^",input2,"$"),bigramDF$unigram)#match input to unigram component of bigram
subbi <- bigramDF[seekbi,]#retrieve matched bigram if present, retrive header if not present
unigramDF$s <- unigramDF$freq/nrow(unigramDF)*0.16#weighted Good-Turing probabability of unigram
useuni <- unigramDF[order(unigramDF$s,decreasing = T),]#ordered weighted Good-Turing probability
useunia <- useuni[1:maxResults,]#top-MaxResults of weighted Good-Turing probability
if (sum(seektri) == 0) {
if(sum(seekbi)==0)
{
return(head(unigramDF[order(unigramDF$freq,decreasing = T),1],maxResults))#return top-maXResults for unigram
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)#weighted Good-Turing probability of bigram
names <- c(subbi$name,useunia$unigram)
score <- c(subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- setdiff(final,profanity)
final <- final[grepl('[[:alpha:]]',final)]
return(final[1:maxResults])
}
subbi$s <- 0.4*subbi$freq/sum(seekbi)#weighted Good-Turing probability of bigram
subtri$s <- subtri$freq/sum(subtri$freq)
names <- c(subtri$name,subbi$name,useunia$unigram)
score <- c(subtri$s,subbi$s,useunia$s)
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord <- predictWord[order(predictWord$score,decreasing = T),]
# in case replicated
final <- unique(predictWord$next_word)
final <- final[1:maxResults]
final <- setdiff(final,profanity)
final <- final[grepl('[[:alpha:]]',final)]
return(final)
}
predict0("new york",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
set.seed(80) #enable reproducibility
Tblogs <- blogs[sample(1:length(blogs),0.2*length(blogs))]
Tnews <- news[sample(1:length(news),0.2*length(news))]
#Ttwitter <- twitter[sample(1:length(twitter),0.05*length(twitter))]
writeLines(Testfile,"./data/TestFile.txt")
Cleanfile <- Corpus(DirSource("./data"))
Cleanfile <- Corpus(VectorSource(Cleanfile))
#corpus <- Corpus(VectorSource(smpl))
Cleanfile <- tm_map(Cleanfile,removePunctuation)
Cleanfile <- tm_map(Cleanfile,removeNumbers)
Cleanfile <- tm_map(Cleanfile,stripWhitespace)
Cleanfile <- tm_map(Cleanfile,content_transformer(tolower))
#Cleanfile <- tm_map(Cleanfile,stemDocument)
Cleanfile <- tm_map(Cleanfile,removeWords, stopwords("english"))
#Read in list of profanity words that we want to remove
profanity <- readLines("./download/profanity.txt",encoding='UTF-8',skipNul=TRUE)
Cleanfile <- tm_map(Cleanfile,removeWords, profanity)
#Cleanfile <- removeSparseTerms(Cleanfile , 0.9995)
Cleanfile <- tm_map(Cleanfile,PlainTextDocument)
save(Cleanfile,file="./corpus/Corpus.RData")
unigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram_token <- function(x)
NGramTokenizer(x, Weka_control(min = 3, max = 3))
unigram <- TermDocumentMatrix(corpus, control=list(tokenize=unigram_token))
save(unigram,file="unigram.RData")
bigram <- TermDocumentMatrix(corpus, control=list(tokenize=bigram_token))
save(bigram,file="bigram.RData")
trigram <- TermDocumentMatrix(corpus, control=list(tokenize=trigram_token))
save(trigram,file="trigram.RData")
freq <- rowapply_simple_triplet_matrix(unigram,sum)
freqbi <- rowapply_simple_triplet_matrix(bigram,sum)
freqtri <- rowapply_simple_triplet_matrix(trigram,sum)
save(freq,file="freq.RData")
save(freqbi,file="freqbi.RData")
save(freqtri ,file="freqtri.RData")
#sort n-grams by frequency
freq_uni <- sort(freq, decreasing = T)
freq_bi <- sort(freqbi, decreasing = T)
freq_tri <- sort(freqtri, decreasing = T)
#to visualize the frequency
barplot(freq_uni[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
barplot(freq_bi[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
barplot(freq_tri[1:20], width = 0.1, main = "Freqency of Top 20 words in unigram")
firstname <- sapply(strsplit(names(freqbi), ' '), function(a) a[1])
secname <- sapply(strsplit(names(freqbi), ' '), function(a) a[2])
firsttriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[1])
sectriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[2])
tritriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[3])
names(freqbi)[1] #view first entry in the bi-gram model
firstname[1]#view the split of the first entry in the bi-gram model into the first word
secname[1]#view the split of the first entry in the bi-gram model into the second  word
#save uni, bi and trigrams as dataframes
unigramDF <- data.frame(names(freq),freq,stringsAsFactors = F)
bigramDF <- data.frame(names(freqbi),freqbi,firstname,secname,stringsAsFactors = F)
trigramDF <- data.frame(names(freqtri),freqtri,paste(firsttriname,sectriname),tritriname,stringsAsFactors = F)
names(unigramDF) <- c('unigram','freq')
names(bigramDF) <- c('bigram','freq','unigram','name')
names(trigramDF) <- c('trigram','freq','bigram','name')
save(unigramDF,bigramDF,trigramDF,file = 'ngram.RData')
predict0("new",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("new york",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("sex and the",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("sex and",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("two weeks",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("case of",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
predict0("two week",profanity,unigramDF, bigramDF, trigramDF, maxResults = 3)
